<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>CRAN - Package tokenizers</title>
<link rel="stylesheet" type="text/css" href="../../CRAN_web.css" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="citation_title" content="A Consistent Interface to Tokenize Natural Language Text [R package tokenizers version 0.1.4]" />
<meta name="citation_author" content="Lincoln Mullen" />
<meta name="citation_publication_date.Published" content="2016-08-29" />
<meta name="citation_public_url" content="https://CRAN.R-project.org/package=tokenizers" />
<meta name="DC.identifier" content="https://CRAN.R-project.org/package=tokenizers" />
<meta name="DC.publisher" content="Comprehensive R Archive Network (CRAN)" />
<style type="text/css">
  table td { vertical-align: top; }
</style>
</head>
<body>
<h2>tokenizers: A Consistent Interface to Tokenize Natural Language Text</h2>
<p>Convert natural language text into tokens. The tokenizers have a
    consistent interface and are compatible with Unicode, thanks to being built
    on the 'stringi' package. Includes tokenizers for shingled n-grams, skip
    n-grams, words, word stems, sentences, paragraphs, characters, lines, and
    regular expressions.</p>
<table summary="Package tokenizers summary">
<tr>
<td>Version:</td>
<td>0.1.4</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.3)</td>
</tr>
<tr>
<td>Imports:</td>
<td><a href="../stringi/index.html">stringi</a> (&ge; 1.0.1), <a href="../Rcpp/index.html">Rcpp</a> (&ge; 0.12.3), <a href="../SnowballC/index.html">SnowballC</a> (&ge; 0.5.1)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td><a href="../Rcpp/index.html">Rcpp</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td><a href="../testthat/index.html">testthat</a>, <a href="../covr/index.html">covr</a>, <a href="../knitr/index.html">knitr</a>, <a href="../rmarkdown/index.html">rmarkdown</a></td>
</tr>
<tr>
<td>Published:</td>
<td>2016-08-29</td>
</tr>
<tr>
<td>Author:</td>
<td>Lincoln Mullen [aut, cre],
  Dmitriy Selivanov [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lincoln Mullen  &#x3c;&#x6c;&#x69;&#x6e;&#x63;&#x6f;&#x6c;&#x6e;&#x20;&#x61;&#x74;&#x20;&#x6c;&#x69;&#x6e;&#x63;&#x6f;&#x6c;&#x6e;&#x6d;&#x75;&#x6c;&#x6c;&#x65;&#x6e;&#x2e;&#x63;&#x6f;&#x6d;&#x3e;</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ropensci/tokenizers/issues">https://github.com/ropensci/tokenizers/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="../../licenses/MIT">MIT</a> + file <a href="LICENSE">LICENSE</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/ropensci/tokenizers">https://github.com/ropensci/tokenizers</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Materials:</td>
<td><a href="README.html">README</a> <a href="news.html">NEWS</a> </td>
</tr>
<tr>
<td>In&nbsp;views:</td>
<td><a href="../../views/NaturalLanguageProcessing.html">NaturalLanguageProcessing</a></td>
</tr>
<tr>
<td>CRAN&nbsp;checks:</td>
<td><a href="../../checks/check_results_tokenizers.html">tokenizers results</a></td>
</tr>
</table>
<h4>Downloads:</h4>
<table summary="Package tokenizers downloads">
<tr>
<td> Reference&nbsp;manual: </td>
<td> <a href="tokenizers.pdf"> tokenizers.pdf </a> </td>
</tr>
<tr>
<td>Vignettes:</td>
<td>
<a href="vignettes/introduction-to-tokenizers.html">Introduction to the tokenizers Package</a><br/>
</td>
</tr>
<tr>
<td> Package&nbsp;source: </td>
<td> <a href="../../../src/contrib/tokenizers_0.1.4.tar.gz"> tokenizers_0.1.4.tar.gz </a> </td>
</tr>
<tr>
<td> Windows&nbsp;binaries: </td>
<td> r-devel: <a href="../../../bin/windows/contrib/3.5/tokenizers_0.1.4.zip">tokenizers_0.1.4.zip</a>, r-release: <a href="../../../bin/windows/contrib/3.4/tokenizers_0.1.4.zip">tokenizers_0.1.4.zip</a>, r-oldrel: <a href="../../../bin/windows/contrib/3.3/tokenizers_0.1.4.zip">tokenizers_0.1.4.zip</a> </td>
</tr>
<tr>
<td> OS&nbsp;X&nbsp;El&nbsp;Capitan&nbsp;binaries: </td>
<td> r-release: <a href="../../../bin/macosx/el-capitan/contrib/3.4/tokenizers_0.1.4.tgz">tokenizers_0.1.4.tgz</a> </td>
</tr>
<tr>
<td> OS&nbsp;X&nbsp;Mavericks&nbsp;binaries: </td>
<td> r-oldrel: <a href="../../../bin/macosx/mavericks/contrib/3.3/tokenizers_0.1.4.tgz">tokenizers_0.1.4.tgz</a> </td>
</tr>
<tr>
<td> Old&nbsp;sources: </td>
<td> <a href="../../../src/contrib/Archive/tokenizers"> tokenizers archive </a> </td>
</tr>
</table>
<h4>Reverse dependencies:</h4>
<table summary="Package tokenizers reverse dependencies">
<tr>
<td>Reverse&nbsp;imports:</td>
<td><a href="../ptstem/index.html">ptstem</a>, <a href="../tidytext/index.html">tidytext</a></td>
</tr>
<tr>
<td>Reverse&nbsp;suggests:</td>
<td><a href="../cleanNLP/index.html">cleanNLP</a></td>
</tr>
</table>
<h4>Linking:</h4>
<p>Please use the canonical form
<a href="https://CRAN.R-project.org/package=tokenizers"><samp>https://CRAN.R-project.org/package=tokenizers</samp></a>
to link to this page.</p>
</body>
</html>
